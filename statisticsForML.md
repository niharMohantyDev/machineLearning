# Statistics Topics for Machine Learning

This document provides an overview of key statistics topics essential for machine learning. Each topic helps in understanding data, improving model accuracy, and validating performance. 

---

## 1. Descriptive Statistics

- **Mean, Median, Mode**: Measures of central tendency that help in summarizing data.
- **Variance and Standard Deviation**: Metrics for data dispersion, indicating spread around the mean.
- **Skewness and Kurtosis**: Metrics that measure symmetry (skewness) and "tailedness" (kurtosis) of a distribution.
- **Percentiles and Quartiles**: Useful for analyzing data distribution, especially with non-normal data.

## 2. Probability and Probability Distributions

- **Basic Probability Rules**: Joint, marginal, and conditional probabilities for understanding data relationships.
- **Bayes' Theorem**: Foundation of Bayesian inference, widely used in models like Naive Bayes.
- **Probability Distributions**: Knowledge of Normal, Binomial, Poisson, and Exponential distributions is critical for modeling.
- **Multivariate Distributions**: Important for analyzing relationships between multiple variables in high-dimensional data.

## 3. Inferential Statistics

- **Confidence Intervals**: Estimates population parameters within a range with a given certainty level.
- **Hypothesis Testing**: Validates model assumptions and results.
- **p-Values**: Quantifies evidence against a null hypothesis, aiding in model validation.
- **ANOVA (Analysis of Variance)**: Compares multiple groups for significant differences.

## 4. Regression Analysis

- **Linear Regression**: Core method for analyzing linear relationships.
- **Logistic Regression**: Used in binary classification problems.
- **Polynomial Regression**: Models non-linear relationships.
- **Regularization (Ridge, Lasso)**: Addresses overfitting and assists in feature selection.

## 5. Bayesian Statistics

- **Bayesian Inference**: Updates beliefs with new evidence, foundational in probabilistic modeling.
- **Markov Chain Monte Carlo (MCMC)**: Samples complex probability distributions.
- **Bayesian Networks**: Graphical models for modeling variable dependencies.

## 6. Dimensionality Reduction

- **Principal Component Analysis (PCA)**: Reduces dimensionality while preserving variance.
- **Singular Value Decomposition (SVD)**: Useful in matrix factorization, especially for recommendation systems.
- **t-SNE and UMAP**: Visualization techniques for high-dimensional data.

## 7. Clustering and Distance Measures

- **K-means, Hierarchical Clustering**: Unsupervised learning methods for finding patterns.
- **Distance Metrics (Euclidean, Manhattan, Cosine)**: Measures essential for clustering and similarity-based learning.

## 8. Time Series Analysis

- **Autocorrelation and Partial Autocorrelation**: Key metrics for understanding data relationships over time.
- **ARIMA and Seasonal Decomposition**: Models for forecasting and handling seasonality.
- **Stationarity**: Ensures reliable model performance by stabilizing time series data.

## 9. Sampling Techniques

- **Random Sampling**: Ensures unbiased data selection for training/testing.
- **Stratified Sampling**: Balances imbalanced data, especially in classification.
- **Bootstrapping**: Resamples data for better metric estimation, useful for validation.

## 10. Evaluation Metrics and Model Validation

- **Confusion Matrix, Precision, Recall, F1 Score**: Core metrics for classification models.
- **ROC-AUC**: Evaluates binary classifier performance.
- **Mean Absolute Error, Mean Squared Error**: Regression model metrics.
- **Cross-Validation**: Reduces overfitting by testing on various data subsets.

## 11. Statistical Significance Testing

- **Chi-Square Test**: Examines independence between categorical variables.
- **t-Test and z-Test**: Compares group differences, useful in A/B testing.
- **Permutation Tests**: Non-parametric testing without assuming normality.

## 12. Feature Selection and Engineering

- **Correlation Analysis**: Identifies variable relationships.
- **Mutual Information**: Measures dependency, aiding in feature selection.
- **Variance Thresholding**: Filters low-variance features to simplify data.

## 13. Resampling Methods

- **Bootstrapping**: Creates multiple samples with replacement for reliable statistics.
- **Jackknife**: Removes one observation at a time to estimate bias and variance.

## 14. Anomaly Detection

- **Z-Scores and IQR for Outliers**: Identifies outliers in data distributions.
- **Statistical Process Control (SPC)**: Monitors data variations over time, useful for quality control.

---

This list covers foundational and advanced statistics concepts that aid in creating, interpreting, and validating machine learning models.
